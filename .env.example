# Inference Backend Configuration

# Backend type (currently only 'llama_cpp' supported)
BACKEND=llama_cpp

# Model configuration
MODEL_PATH=/path/to/your/model.gguf

# llama.cpp specific settings
N_CTX=2048                    # Context length (max tokens)
N_GPU_LAYERS=-1               # GPU layers (-1 = use all available)
N_THREADS=8                   # CPU threads (leave unset for auto)
TEMPERATURE=0.7               # Sampling temperature
TOP_P=0.95                    # Top-p sampling
REPEAT_PENALTY=1.1            # Repetition penalty

# Service configuration
HOST=0.0.0.0
PORT=8000

# Logging
LOG_LEVEL=INFO